# -*- coding: utf-8 -*-
"""EECS595 Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cUoc4L7REGTl5nJ4qtGcjtcsY3a_wg8A
"""

import pandas as pd
from transformers import BertModel
from transformers import BertTokenizer
import torch
import torch.nn as nn
import numpy as np
from transformers import AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel
import nltk
import jsonlines
from nltk.corpus import words
from nltk.corpus import wordnet 
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import random
import time
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler



class DataType:
    string_type = ['tweet', 'description', 'user_location']
    float_type = ['num_tag', 'num_emoji', 'follower_count', 'friends_count', 'retweet_count', 'num_at']
    boolean_value = ['has_number', 'has_mark', 'has_link']

class Input2Model:
    def __init__(self, df):
        self.df = df
        self.feature_input = []
        self.tweet_max_length = 128
        self.bio_max_length = 128
        self.location_max_length = 16

    def clean_tweet(self):
        nltk.download('words')
        nltk.download('wordnet')
        nltk.download('stopwords')
        nltk.download('punkt')

        word_net = set(wordnet.words())
        word_ = set(words.words())
        total_words = set.union(word_net,word_)
        stop_words = set(stopwords.words('english'))


        for index, row in tqdm(self.df.iterrows()):
            text = row[4]
            text_tokens = word_tokenize(text)
            tokens_without_sw = [word for word in text_tokens if not word.lower() in stopwords.words()]
            filtered_sentence = (" ").join(tokens_without_sw)
            
        # for text in tqdm(self.tweet):
            num_words = len(text.split())
            newtext = " ".join(w for w in nltk.wordpunct_tokenize(text) \
                    if w.lower() in total_words or not w.isalpha())
            num_english = len(newtext.split())
            if num_english/num_words < 0.75:
                
                df.drop(index, inplace=True)

    def model_input(self):
        for each in self.df.text.values:
            tweet_features = Tweet2Features(each)
            num_tag, has_num, has_mark, num_at, num_emoji = tweet_features.feature_style()
            model_feature_input = {
                'num_emoji': num_emoji, # float
                'num_tag': num_tag,  # float
                'num_at': num_at, # float
                'has_num': has_num, # bool
                'has_mark': has_mark, # bool
            }
            self.feature_input.append(model_feature_input)
        meta_info_df = self.df[['text', 'description', 'user_location', 'follower_count', 'friends_count', 'retweet_count', 'has_link', 'num_of_likes']]
        feature_input_df = pd.DataFrame(self.feature_input)
        meta_info_df = meta_info_df.reset_index(drop=True)
        model_input_df = pd.concat([meta_info_df, feature_input_df], axis=1)
        train_df, val_df = train_test_split(model_input_df, test_size=0.1, random_state=2020)
        return train_df, val_df

    def preprocess_for_BERT(self, given_df):
        tweet_input_ids = []
        tweet_attention_masks = []
        user_bio_ids = []
        user_bio_attention_masks = []
        user_location_ids = []
        user_location_attention_masks = []
        tweet = given_df.text.values
        user_bio = given_df.description.values
        user_location = given_df.user_location.values
        
        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
        for i in range(len(tweet)):
            encoded_tweet = tokenizer.encode_plus(
                text=tweet[i],    # Preprocess sentence
                add_special_tokens=True,          # Add `[CLS]` and `[SEP]`
                max_length=self.tweet_max_length, # Max length to truncate/pad
                pad_to_max_length=True,           # Pad sentence to max length
                return_attention_mask=True        # Return attention mask
            )
            tweet_input_ids.append(encoded_tweet.get('input_ids'))
            tweet_attention_masks.append(encoded_tweet.get('attention_mask'))

            encoded_user_bio = tokenizer.encode_plus(
                text=user_bio[i],    # Preprocess sentence
                add_special_tokens=True,          # Add `[CLS]` and `[SEP]`
                max_length=self.bio_max_length, # Max length to truncate/pad
                pad_to_max_length=True,           # Pad sentence to max length
                return_attention_mask=True        # Return attention mask
            )
            user_bio_ids.append(encoded_user_bio.get('input_ids'))
            user_bio_attention_masks.append(encoded_user_bio.get('attention_mask'))

            encoded_user_location = tokenizer.encode_plus(
                text=user_location[i],    # Preprocess sentence
                add_special_tokens=True,          # Add `[CLS]` and `[SEP]`
                max_length=self.location_max_length, # Max length to truncate/pad
                pad_to_max_length=True,           # Pad sentence to max length
                return_attention_mask=True        # Return attention mask
            )
            user_location_ids.append(encoded_user_location.get('input_ids'))
            user_location_attention_masks.append(encoded_user_location.get('attention_mask'))

        return tweet_input_ids, tweet_attention_masks, user_bio_ids, user_bio_attention_masks, user_location_ids, user_location_attention_masks

import numpy as np

class TweetDataset(torch.utils.data.Dataset):
    def __init__(self, tweet_input_ids, tweet_attention_masks, user_bio_ids, user_bio_attention_masks, user_location_ids, user_location_attention_masks, df):
        self.tweet_input_ids = tweet_input_ids
        self.tweet_attention_masks = tweet_attention_masks
        self.user_bio_ids = user_bio_ids
        self.user_bio_attention_masks = user_bio_attention_masks
        self.user_location_ids = user_location_ids
        self.user_location_attention_masks = user_location_attention_masks
        self.num_emoji = df.num_emoji.values
        self.follower_count = df.follower_count.values
        self.has_mark = df.has_mark.values 
        self.has_num = df.has_num.values
        self.retweet_count = df.retweet_count.values
        self.num_at = df.num_at.values
        self.has_link = df.has_link.values
        self.num_tag = df.num_tag.values
        self.num_of_likes = df.num_of_likes.values
        self.friends_count = df.friends_count.values 
    
    def __len__(self):
        return len(self.tweet_input_ids)
    
    def __getitem__(self, index):
        tweet_ids = np.asarray(self.tweet_input_ids[index]).reshape(-1,1)
        tweet_attn = np.asarray(self.tweet_attention_masks[index]).reshape(-1,1)
        tweet = (tweet_ids, tweet_attn)

        user_bio_ids = np.asarray(self.user_bio_ids[index]).reshape(-1,1)
        user_bio_attn = np.asarray(self.user_bio_attention_masks[index]).reshape(-1,1)
        user_bio = (user_bio_ids, user_bio_attn)

        user_location_ids = np.asarray(self.user_location_ids[index]).reshape(-1,1)
        user_location_attn = np.asarray(self.user_location_attention_masks[index]).reshape(-1,1)
        user_location = (user_location_ids, user_location_attn)
      
        y = self.num_of_likes[index]

        return {'tweet': tweet, 'description':user_bio, 'user_location':user_location, 'num_emoji': self.num_emoji[index], 'follower_count': self.follower_count[index], 'has_mark': self.has_mark[index], 'has_num': self.has_num[index], 'retweet_count': self.retweet_count[index],
                'num_at': self.num_at[index], 'friends_count': self.friends_count[index], 'has_link': self.has_link[index], 'num_tag': self.num_tag[index], 'num_of_likes': y}

class BertClassifier(nn.Module):
    def __init__(self, num_string, num_float, num_bool, output_dimension, freeze_bert=False):

        super(BertClassifier, self).__init__()

        float_hidden, float_out = 8, 16
        bool_out_size = 8
        self.string2emb = RobertaModel.from_pretrained('roberta-base', add_pooling_layer=True)
        self.float2emb = nn.Sequential(
            nn.Linear(1, float_hidden),
            nn.ReLU(),
            nn.Linear(float_hidden, float_out),
        )
        self.bool2emb = nn.Linear(num_bool, bool_out_size)
        # 768, 1024
        output_size = 768 * num_string + float_out * num_float
        if num_bool != 0:
            output_size += 8
        h1, h2, h3 = 1024, 256, 64

        self.classifier = nn.Sequential(
            nn.Linear(output_size, h1),
            nn.ReLU(),
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Linear(h2, h3),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(h3, output_dimension)
        )
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
        # layers

    def forward(self, batch, selected_keys):
        '''
        each batch is
        {(input_ids, attention_mask)
            'tweet': [[input_ids1, input_ids2 ...], [attention_mask1, attention_mask2, ...]], # string
            'abstract': [abstract1, abstract2, abstract3, ...], # string
            'text_feature1': [True, False, True],
            'text_feature2': [True, False, True],
            'text_feature3': [True, False, True],
            'publication_year': float, # every int should be returned to float
        }
        '''
        bool_concat = []
        first = True
        datatype = DataType()
        output = torch.empty((2,2))
        # print(batch)
        for key, value in batch.items():
            if key in selected_keys:
                # print(key)
                if key in datatype.float_type:
                    value = value.float().to(device)
                    float_emb = self.float2emb(value.reshape(-1,1))
                    if first:
                        output = float_emb
                        first = False
                    else:
                        output = torch.cat((output, float_emb),1)
                elif key in datatype.string_type:
                    # print("enter string")
                    string_emb = self.string2emb(input_ids=value[0][:,:,0].to(device), attention_mask=value[1][:,:,0].to(device))
                    last_hidden_state_cls = string_emb[0][:, 0, :]
                    if first:
                        # print("enter first")
                        output = last_hidden_state_cls
                        first = False
                    else:
                        # print("enter second")
                        output = torch.cat((output, last_hidden_state_cls),1)
                else:
                    bool_concat.append(value.float())
    
        if bool_concat:
            num_bool = len(bool_concat)
            batch_size = len(bool_concat[0])
            bool_array = torch.empty([num_bool,batch_size])
            # print(bool_array.size())
            for i in range(num_bool):
                # temp = np.array(bool_concat[i])
                  bool_array[i] = torch.from_numpy(np.array(bool_concat[i]))
            # bool_concat = np.array(bool_concat).astype(int)
            # bool_concat = np.array(bool_concat)
            # print(bool_concat)
            # bool_concat = torch.from_numpy(np.array(bool_concat))
            bool_array = bool_array.to(device)
            # print(bool_array)
            # print(torch.transpose(bool_array,0,1))
            bool_emb = self.bool2emb(torch.transpose(bool_array,0,1))
            output = output = torch.cat((output, bool_emb),1)
        labels = batch['num_of_likes'].to(device)
        logits = self.classifier(output)
        return logits, labels

class Tweet2Features:
    def __init__(self, tweet):
        self.tweet = tweet
        # self.tokens = word_tokenize(title)

    def feature_style(self):
        import re

    def feature_style(self):
        pattern_tag = '#\s\w+'
        pattern_number = '\d'
        pattern_mark = '[\?\!]'
        pattern_at = '@\s[a-zA-Z0-9]+'
        pattern_emoji = re.compile('[#*0-9]ï¸âƒ£|[Â©Â®â€¼â‰â„¢â„¹â†”-â†™â†©â†ªâŒšâŒ›âŒ¨ââ©-â³â¸-âºâ“‚â–ªâ–«â–¶â—€â—»-â—¾â˜€-â˜„â˜Žâ˜‘â˜”â˜•â˜˜]|â˜[ðŸ»-ðŸ¿]?|[â˜ â˜¢â˜£â˜¦â˜ªâ˜®â˜¯â˜¸-â˜ºâ™€â™‚â™ˆ-â™“â™Ÿâ™ â™£â™¥â™¦â™¨â™»â™¾â™¿âš’-âš—âš™âš›âšœâš âš¡âšªâš«âš°âš±âš½âš¾â›„â›…â›ˆâ›Žâ›â›‘â›“â›”â›©â›ªâ›°-â›µâ›·â›¸]|â›¹(?:ï¸â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[â›ºâ›½âœ‚âœ…âœˆâœ‰]|[âœŠ-âœ][ðŸ»-ðŸ¿]?|[âœâœ’âœ”âœ–âœâœ¡âœ¨âœ³âœ´â„â‡âŒâŽâ“-â•â—â£â¤âž•-âž—âž¡âž°âž¿â¤´â¤µâ¬…-â¬‡â¬›â¬œâ­â­•ã€°ã€½ãŠ—ãŠ™ðŸ€„ðŸƒðŸ…°ðŸ…±ðŸ…¾ðŸ…¿ðŸ†ŽðŸ†‘-ðŸ†š]|ðŸ‡¦[ðŸ‡¨-ðŸ‡¬ðŸ‡®ðŸ‡±ðŸ‡²ðŸ‡´ðŸ‡¶-ðŸ‡ºðŸ‡¼ðŸ‡½ðŸ‡¿]|ðŸ‡§[ðŸ‡¦ðŸ‡§ðŸ‡©-ðŸ‡¯ðŸ‡±-ðŸ‡´ðŸ‡¶-ðŸ‡¹ðŸ‡»ðŸ‡¼ðŸ‡¾ðŸ‡¿]|ðŸ‡¨[ðŸ‡¦ðŸ‡¨ðŸ‡©ðŸ‡«-ðŸ‡®ðŸ‡°-ðŸ‡µðŸ‡·ðŸ‡º-ðŸ‡¿]|ðŸ‡©[ðŸ‡ªðŸ‡¬ðŸ‡¯ðŸ‡°ðŸ‡²ðŸ‡´ðŸ‡¿]|ðŸ‡ª[ðŸ‡¦ðŸ‡¨ðŸ‡ªðŸ‡¬ðŸ‡­ðŸ‡·-ðŸ‡º]|ðŸ‡«[ðŸ‡®-ðŸ‡°ðŸ‡²ðŸ‡´ðŸ‡·]|ðŸ‡¬[ðŸ‡¦ðŸ‡§ðŸ‡©-ðŸ‡®ðŸ‡±-ðŸ‡³ðŸ‡µ-ðŸ‡ºðŸ‡¼ðŸ‡¾]|ðŸ‡­[ðŸ‡°ðŸ‡²ðŸ‡³ðŸ‡·ðŸ‡¹ðŸ‡º]|ðŸ‡®[ðŸ‡¨-ðŸ‡ªðŸ‡±-ðŸ‡´ðŸ‡¶-ðŸ‡¹]|ðŸ‡¯[ðŸ‡ªðŸ‡²ðŸ‡´ðŸ‡µ]|ðŸ‡°[ðŸ‡ªðŸ‡¬-ðŸ‡®ðŸ‡²ðŸ‡³ðŸ‡µðŸ‡·ðŸ‡¼ðŸ‡¾ðŸ‡¿]|ðŸ‡±[ðŸ‡¦-ðŸ‡¨ðŸ‡®ðŸ‡°ðŸ‡·-ðŸ‡»ðŸ‡¾]|ðŸ‡²[ðŸ‡¦ðŸ‡¨-ðŸ‡­ðŸ‡°-ðŸ‡¿]|ðŸ‡³[ðŸ‡¦ðŸ‡¨ðŸ‡ª-ðŸ‡¬ðŸ‡®ðŸ‡±ðŸ‡´ðŸ‡µðŸ‡·ðŸ‡ºðŸ‡¿]|ðŸ‡´ðŸ‡²|ðŸ‡µ[ðŸ‡¦ðŸ‡ª-ðŸ‡­ðŸ‡°-ðŸ‡³ðŸ‡·-ðŸ‡¹ðŸ‡¼ðŸ‡¾]|ðŸ‡¶ðŸ‡¦|ðŸ‡·[ðŸ‡ªðŸ‡´ðŸ‡¸ðŸ‡ºðŸ‡¼]|ðŸ‡¸[ðŸ‡¦-ðŸ‡ªðŸ‡¬-ðŸ‡´ðŸ‡·-ðŸ‡¹ðŸ‡»ðŸ‡½-ðŸ‡¿]|ðŸ‡¹[ðŸ‡¦ðŸ‡¨ðŸ‡©ðŸ‡«-ðŸ‡­ðŸ‡¯-ðŸ‡´ðŸ‡·ðŸ‡¹ðŸ‡»ðŸ‡¼ðŸ‡¿]|ðŸ‡º[ðŸ‡¦ðŸ‡¬ðŸ‡²ðŸ‡³ðŸ‡¸ðŸ‡¾ðŸ‡¿]|ðŸ‡»[ðŸ‡¦ðŸ‡¨ðŸ‡ªðŸ‡¬ðŸ‡®ðŸ‡³ðŸ‡º]|ðŸ‡¼[ðŸ‡«ðŸ‡¸]|ðŸ‡½ðŸ‡°|ðŸ‡¾[ðŸ‡ªðŸ‡¹]|ðŸ‡¿[ðŸ‡¦ðŸ‡²ðŸ‡¼]|[ðŸˆðŸˆ‚ðŸˆšðŸˆ¯ðŸˆ²-ðŸˆºðŸ‰ðŸ‰‘ðŸŒ€-ðŸŒ¡ðŸŒ¤-ðŸŽ„]|ðŸŽ…[ðŸ»-ðŸ¿]?|[ðŸŽ†-ðŸŽ“ðŸŽ–ðŸŽ—ðŸŽ™-ðŸŽ›ðŸŽž-ðŸ]|ðŸ‚[ðŸ»-ðŸ¿]?|[ðŸƒðŸ„](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ…ðŸ†]|ðŸ‡[ðŸ»-ðŸ¿]?|[ðŸˆðŸ‰]|ðŸŠ(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ‹ðŸŒ](?:ï¸â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ-ðŸ°]|ðŸ³(?:ï¸â€ðŸŒˆ)?|ðŸ´(?:â€â˜ ï¸|ó §ó ¢(?:ó ¥ó ®ó §|ó ³ó £ó ´|ó ·ó ¬ó ³)ó ¿)?|[ðŸµðŸ·-ðŸ‘€]|ðŸ‘(?:ï¸â€ðŸ—¨ï¸)?|[ðŸ‘‚ðŸ‘ƒ][ðŸ»-ðŸ¿]?|[ðŸ‘„ðŸ‘…]|[ðŸ‘†-ðŸ‘][ðŸ»-ðŸ¿]?|[ðŸ‘‘-ðŸ‘¥]|[ðŸ‘¦ðŸ‘§][ðŸ»-ðŸ¿]?|ðŸ‘¨(?:â€(?:[âš•âš–âœˆ]ï¸|â¤ï¸â€(?:ðŸ’‹â€)?ðŸ‘¨|[ðŸŒ¾ðŸ³ðŸŽ“ðŸŽ¤ðŸŽ¨ðŸ«ðŸ­]|ðŸ‘¦(?:â€ðŸ‘¦)?|ðŸ‘§(?:â€[ðŸ‘¦ðŸ‘§])?|[ðŸ‘¨ðŸ‘©]â€(?:ðŸ‘¦(?:â€ðŸ‘¦)?|ðŸ‘§(?:â€[ðŸ‘¦ðŸ‘§])?)|[ðŸ’»ðŸ’¼ðŸ”§ðŸ”¬ðŸš€ðŸš’ðŸ¦°-ðŸ¦³])|[ðŸ»-ðŸ¿](?:â€(?:[âš•âš–âœˆ]ï¸|[ðŸŒ¾ðŸ³ðŸŽ“ðŸŽ¤ðŸŽ¨ðŸ«ðŸ­ðŸ’»ðŸ’¼ðŸ”§ðŸ”¬ðŸš€ðŸš’ðŸ¦°-ðŸ¦³]))?)?|ðŸ‘©(?:â€(?:[âš•âš–âœˆ]ï¸|â¤ï¸â€(?:ðŸ’‹â€)?[ðŸ‘¨ðŸ‘©]|[ðŸŒ¾ðŸ³ðŸŽ“ðŸŽ¤ðŸŽ¨ðŸ«ðŸ­]|ðŸ‘¦(?:â€ðŸ‘¦)?|ðŸ‘§(?:â€[ðŸ‘¦ðŸ‘§])?|ðŸ‘©â€(?:ðŸ‘¦(?:â€ðŸ‘¦)?|ðŸ‘§(?:â€[ðŸ‘¦ðŸ‘§])?)|[ðŸ’»ðŸ’¼ðŸ”§ðŸ”¬ðŸš€ðŸš’ðŸ¦°-ðŸ¦³])|[ðŸ»-ðŸ¿](?:â€(?:[âš•âš–âœˆ]ï¸|[ðŸŒ¾ðŸ³ðŸŽ“ðŸŽ¤ðŸŽ¨ðŸ«ðŸ­ðŸ’»ðŸ’¼ðŸ”§ðŸ”¬ðŸš€ðŸš’ðŸ¦°-ðŸ¦³]))?)?|[ðŸ‘ª-ðŸ‘­]|ðŸ‘®(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ‘¯(?:â€[â™€â™‚]ï¸)?|ðŸ‘°[ðŸ»-ðŸ¿]?|ðŸ‘±(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ‘²[ðŸ»-ðŸ¿]?|ðŸ‘³(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ‘´-ðŸ‘¶][ðŸ»-ðŸ¿]?|ðŸ‘·(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ‘¸[ðŸ»-ðŸ¿]?|[ðŸ‘¹-ðŸ‘»]|ðŸ‘¼[ðŸ»-ðŸ¿]?|[ðŸ‘½-ðŸ’€]|[ðŸ’ðŸ’‚](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ’ƒ[ðŸ»-ðŸ¿]?|ðŸ’„|ðŸ’…[ðŸ»-ðŸ¿]?|[ðŸ’†ðŸ’‡](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ’ˆ-ðŸ’©]|ðŸ’ª[ðŸ»-ðŸ¿]?|[ðŸ’«-ðŸ“½ðŸ“¿-ðŸ”½ðŸ•‰-ðŸ•ŽðŸ•-ðŸ•§ðŸ•¯ðŸ•°ðŸ•³]|ðŸ•´[ðŸ»-ðŸ¿]?|ðŸ•µ(?:ï¸â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ•¶-ðŸ•¹]|ðŸ•º[ðŸ»-ðŸ¿]?|[ðŸ–‡ðŸ–Š-ðŸ–]|[ðŸ–ðŸ–•ðŸ––][ðŸ»-ðŸ¿]?|[ðŸ–¤ðŸ–¥ðŸ–¨ðŸ–±ðŸ–²ðŸ–¼ðŸ—‚-ðŸ—„ðŸ—‘-ðŸ—“ðŸ—œ-ðŸ—žðŸ—¡ðŸ—£ðŸ—¨ðŸ—¯ðŸ—³ðŸ—º-ðŸ™„]|[ðŸ™…-ðŸ™‡](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ™ˆ-ðŸ™Š]|ðŸ™‹(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ™Œ[ðŸ»-ðŸ¿]?|[ðŸ™ðŸ™Ž](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ™[ðŸ»-ðŸ¿]?|[ðŸš€-ðŸš¢]|ðŸš£(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸš¤-ðŸš³]|[ðŸš´-ðŸš¶](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸš·-ðŸš¿]|ðŸ›€[ðŸ»-ðŸ¿]?|[ðŸ›-ðŸ›…ðŸ›‹]|ðŸ›Œ[ðŸ»-ðŸ¿]?|[ðŸ›-ðŸ›’ðŸ› -ðŸ›¥ðŸ›©ðŸ›«ðŸ›¬ðŸ›°ðŸ›³-ðŸ›¹ðŸ¤-ðŸ¤—]|[ðŸ¤˜-ðŸ¤œ][ðŸ»-ðŸ¿]?|ðŸ¤|[ðŸ¤žðŸ¤Ÿ][ðŸ»-ðŸ¿]?|[ðŸ¤ -ðŸ¤¥]|ðŸ¤¦(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ¤§-ðŸ¤¯]|[ðŸ¤°-ðŸ¤¶][ðŸ»-ðŸ¿]?|ðŸ¤·(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ¤¸ðŸ¤¹](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|ðŸ¤º|ðŸ¤¼(?:â€[â™€â™‚]ï¸)?|[ðŸ¤½ðŸ¤¾](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ¥€-ðŸ¥…ðŸ¥‡-ðŸ¥°ðŸ¥³-ðŸ¥¶ðŸ¥ºðŸ¥¼-ðŸ¦¢ðŸ¦°-ðŸ¦´]|[ðŸ¦µðŸ¦¶][ðŸ»-ðŸ¿]?|ðŸ¦·|[ðŸ¦¸ðŸ¦¹](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ§€-ðŸ§‚ðŸ§]|[ðŸ§‘-ðŸ§•][ðŸ»-ðŸ¿]?|ðŸ§–(?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ§—-ðŸ§](?:â€[â™€â™‚]ï¸|[ðŸ»-ðŸ¿](?:â€[â™€â™‚]ï¸)?)?|[ðŸ§žðŸ§Ÿ](?:â€[â™€â™‚]ï¸)?|[ðŸ§ -ðŸ§¿]')

        num_tag = len(re.findall(pattern_tag, self.tweet))
        has_num = re.search(pattern_number, self.tweet)!=None
        has_mark = re.search(pattern_mark, self.tweet)!=None
        num_at = len(re.findall(pattern_at, self.tweet))
        num_emoji = len(re.findall(pattern_emoji, self.tweet))

        return num_tag, has_num, has_mark, num_at, num_emoji


    def feature_topic(self, pre_calculated_features):
        # pre_calculated_features is an instance of FeaturesByNLPModels()
        return {
            'title2subarea': pre_calculated_features.title2subarea[self.title],
        }

    def feature_novelty(self, common_word_set):
        # common_word_set is what you pre-calculate on all paper title+abstract (tokenized version) of words larger than 100 occurrences
        # import nltk
        # new_text = nltk.word_tokenize(text)
        # We should avoid double tokenization: new_new_text = nltk.word_tokenize(new_text)

        low_freq_words = {i for i in self.tokens if i not in common_word_set}
        num_low_freq_words = len(low_freq_words)
        return {
            'num_low_freq_words': num_low_freq_words,
        }  # a number that shows how many low-frequency words are in the title.

def initialize_model(num_string, num_float, num_bool, output_dimension, epochs=4):
    """Initialize the Bert Classifier, the optimizer and the learning rate scheduler.
    """
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(num_string, num_float, num_bool, output_dimension, freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=1e-5,    # Default learning rate
                      eps=1e-4,    # Default epsilon value
                      weight_decay=1e-3
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler

def set_seed(seed_value=42):
    """Set seed for reproducibility.
    """
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)

def train(model, train_dataloader, selected_keys, val_dataloader=None, epochs=4, evaluation=False):
    """Train the BertClassifier model.
    """
    # Start training loop
    print("Start training...\n")
    for epoch_i in range(epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        print(f"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}")
        print("-"*70)

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            # b_input_ids, b_attn_mask, a_input_ids, a_attn_mask, train_meta, b_labels = tuple(t.to(device) for t in batch)
            # Zero out any previously calculated gradients
            model.zero_grad()
            # Perform a forward pass. This will return logits.
            logits, labels = model(batch, selected_keys)

            # Compute loss and accumulate the loss values
            # labels = batch['num_of_likes'].to(device)
            loss = loss_fn(logits, labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent "exploding gradients"
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                print(f"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}")

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        print("-"*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader, selected_keys)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            print(f"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}")
            print("-"*70)
        print("\n")
    
    print("Training complete!")


def evaluate(model, val_dataloader, selected_keys):
    """After the completion of each training epoch, measure the model's performance
    on our validation set.
    """
    # Put the model into the evaluation mode. The dropout layers are disabled during
    # the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        # b_input_ids, b_attn_mask, a_input_ids, a_attn_mask, val_meta, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits, labels = model(batch, selected_keys)

        # labels = batch['num_of_likes'].to(device)
        # Compute loss
        loss = loss_fn(logits, labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

if __name__ == "__main__": 
    if torch.cuda.is_available():       
        device = torch.device("cuda")
        print(f'There are {torch.cuda.device_count()} GPU(s) available.')
        print('Device name:', torch.cuda.get_device_name(0))

    else:
        print('No GPU available, using the CPU instead.')
        device = torch.device("cpu")

    df = pd.read_json('cleaned_tweet.jsonl', lines=True)
    df.drop_duplicates(subset ="text", keep = False, inplace = True)
    # loss_fn = nn.CrossEntropyLoss()

    df = df[df['num_of_likes'] < 4000]
    df = df[df['num_of_likes'] > 100]
    # df['num_of_likes'].describe()
    df['num_of_likes'] = (df['num_of_likes'] >= 356).astype(int)

    input2model = Input2Model(df)

    train_df, val_df = input2model.model_input()
    train_tweet_input_ids, train_tweet_attention_masks, train_user_bio_ids, train_user_bio_attention_masks, train_user_location_ids, train_user_location_attention_masks = input2model.preprocess_for_BERT(train_df)
    val_tweet_input_ids, val_tweet_attention_masks, val_user_bio_ids, val_user_bio_attention_masks, val_user_location_ids, val_user_location_attention_masks = input2model.preprocess_for_BERT(val_df)

    from sklearn import preprocessing
    train_df = train_df.apply(preprocessing.LabelEncoder().fit_transform)
    train_df

    from sklearn.feature_selection import SelectKBest
    from sklearn.feature_selection import chi2

    # target_col = 'num_of_likes'
    # x = train_df[['follower_count', 'friends_count', 'retweet_count', 'has_link', 'num_emoji', 'num_tag', 'num_at', 'has_num', 'has_mark']]
    # y = train_df[target_col]
    # # y = train_df.iloc[:, train_df.columns == target_col]

    # select = SelectKBest(score_func=chi2, k=5)
    # z = select.fit_transform(x,y)
    # print(z)

    batch_size = 32
    train_data = TweetDataset(train_tweet_input_ids, train_tweet_attention_masks, train_user_bio_ids, train_user_bio_attention_masks, train_user_location_ids, train_user_location_attention_masks, train_df)

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)

    val_data = TweetDataset(val_tweet_input_ids, val_tweet_attention_masks, val_user_bio_ids, val_user_bio_attention_masks, val_user_location_ids, val_user_location_attention_masks, val_df)

    val_sampler = RandomSampler(val_data)
    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, drop_last=True)

    selected_keys = ['tweet']

    print(selected_keys)
    print("lr = 1e-5, eps = 1e-4, weightdecay = 1e-3, numwarmup = h1=1024, h2=256, h3 = 64, batch = 32")

    loss_fn = nn.CrossEntropyLoss()
    set_seed(42)    # Set seed for reproducibility
    bert_classifier, optimizer, scheduler = initialize_model(num_string = 1, num_float = 0, num_bool = 0, output_dimension = 2)
    train(bert_classifier, train_dataloader, selected_keys, val_dataloader, epochs=6, evaluation=True)

